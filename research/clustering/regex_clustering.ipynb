{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster analysis** or **clustering** is the task of grouping a set of objects in such a way that objects in the same cluster are more similar to each other according to some similarity function than to those in other clusters.\n",
    "\n",
    "Let there be some set of objects $I$ belonging to some space $R$. If there exists such a function $f_{enc}$ that is a mapping of an object from space $R$ to $R'$, then it is possible to define a set of objects $I'$ belonging to some space $R'$.\n",
    "\n",
    "Let us assume that there is some function $f_{sim}$ that is able to show the similarity measure $sim$ of two objects belonging to the space $R$. However, we do not know the function $f_{sim}$. But if we define a function $f_{sim}'$ that is able to show the similarity measure $sim'$ of from two objects $I_{1}'$ and $I_{2}'$ belonging to the space $R'$, then we can assume that the result of calculating the similarity measure $sim$ from from two objects $I_{1}$ and $I_{2}$ can be interpreted with some accuracy as the result of an unknown $f_{sim}$.\n",
    "\n",
    "Knowing all this, we can find clusters, that is, sets of the most similar objects of set $I'$ according to the degree of measure of similarity $sim'$ in space $R'$, which can show with some accuracy the similarity of objects of set $I$ in space $R$. Let's assume that close objects are those objects that have a $sim$ greater than a certain threshold $t_{sim}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expression case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of regular expressions, the space $R$ is an alphabet of valid symbols for all available dialects of regular expressions, collected in a set $I$. The set of regular expressions $I$ is collected from open sources using parsing. The unknown $f_{sim}$ can show the similarity measure $sim$ of regular expressions by various characteristics, for example, by structure. In order to cluster the collected set $I$, it is necessary to define:\n",
    "\n",
    "- function $f_{enc}$\n",
    "- space $R'$ (its limitations)\n",
    "- function $f_{sim}'$ and $t_{sim}$\n",
    "\n",
    "It can be said that the threshold $t_{sim}$ and $f_{sim}'$is usually determined by the chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1\n",
    "\n",
    "```\n",
    "Encoder: TF-IDF Vectorizer\n",
    "Algorithm: KMeans\n",
    "```\n",
    "\n",
    "TF (Term Frequency) measures how often a certain word appears in a given document. Thus, TF measures the importance of a word in the context of a single document.\n",
    "\n",
    "IDF (Inverse Document Frequency) measures how unique a word is across a collection of documents. Words that appear in most documents have a low IDF because they do not contribute much information value.\n",
    "\n",
    "The TF-IDF formula combines the concepts of TF and IDF to calculate the importance of each word in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize by chars\n",
    "\n",
    "First, we tokenize each regular expression simply by chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize by non-terminals\n",
    "\n",
    "Suppose that the similarity of two regular expressions can be determined only by non-terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to tokens and tokenize\n",
    "\n",
    "Finally, let's try to do tokenization by replacing symbols with tokens of some regular expressions construction (like compilers).\n",
    "\n",
    "Constructions that are described by one symbol will not affect the TF-IDF. However, constructions that use 2 or more symbols for their description will change the TF-IDF score.\n",
    "\n",
    "Of course, there are many dialects of regular expressions and the syntax of different engines differs from each other. However, in most cases they have similar constructions. These rules for obtaining tokens are based on the syntax of the most common dialect - <a href=\"https://www.pcre.org/original/doc/html/pcrepattern.html\">PCRE</a>.\n",
    "\n",
    "**Example of token rules (1 symbol)**:\n",
    "- \\| -> alt\n",
    "- \\. -> any\n",
    "- \\? -> quant_0_1\n",
    "- \\+ -> quant_1_more\n",
    "\n",
    "Note that two-character constructions most often contain multiple tokens of one character. However, such constructions are most often immediately interpreted by the regular expression engine as a single token.\n",
    "\n",
    "**Example of token rules (2 or more symbols)**:\n",
    "- \\\\d -> decimal_digit\n",
    "- \\\\D -> not_decimal_digit\n",
    "- \\\\h -> horizontal_white_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11",
   "language": "python",
   "name": "3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
